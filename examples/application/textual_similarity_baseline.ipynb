{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 『2022信通院兴智杯：深度学习模型可解释竞赛』- 文本相似度可解释性评测\n",
    "## 1、项目介绍\n",
    "深度学习模型在很多NLP任务上已经取得巨大成功，但其常被当作一个黑盒使用，内部预测机制对使用者是不透明的。这使得深度学习模型结果不被使用者信任，增加了落地难度，尤其在医疗、法律等特殊领域。同时，当模型出现效果不好或鲁棒性差等问题时，由于不了解其内部机制，很难对模型进行改进优化。近期，深度学习模型的可解释性被越来越多的人关注。但模型的可解释性评估还不够完善，本基线提供了文本相似度任务的评测数据和相关评测指标，旨在评估模型的可解释性。\n",
    "## 2、基线运行\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 依赖安装\n",
    "安装一些必须的依赖包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install paddlepaddle-gpu\n",
    "!pip3 install -U paddlenlp==2.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据准备\n",
    "#### 1）模型训练数据\n",
    "我们推荐使用LCQMC数据集训练中文相似度计算模型。Paddlenlp框架会自动下载及缓存训练数据集，默认缓存存储路径为\"~/.paddlenlp/datasets\"。如需修改训练数据，请参考『初始化工作』中DATASET_NAME的修改。\n",
    "#### 2）下载预训练模型\n",
    "基线使用了ERNIE-3.0-base预训练模型。Paddlenlp框架自动缓存模型文件，默认缓存存储路径为\"~/.paddlenlp/models\"。如需修改依赖的预训练模型，请在『初始化工作』中修改MODEL_NAME。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 初始化工作\n",
    "初始化工作包括了模型选择及加载、训练数据集选择、模型存储路径设定、抽取证据的长度占原文本长度的比例设定等。可按需更改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddlenlp\n",
    "from paddlenlp.transformers import ErnieForSequenceClassification, ErnieTokenizer\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "# Select pre-trained model\n",
    "MODEL_NAME = \"ernie-3.0-base-zh\" # choose from [\"ernie-1.0\", \"ernie-1.0-base-zh\", \"ernie-1.0-large-zh-cw\", \"ernie-2.0-base-zh\", \"ernie-2.0-large-zh\", \"ernie-3.0-xbase-zh\", \"ernie-3.0-base-zh\", \"ernie-3.0-medium-zh\", \"ernie-3.0-mini-zh\", \"ernie-3.0-micro-zh\", \"ernie-3.0-nano-zh\"]\n",
    "# Select dataset for model training\n",
    "DATASET_NAME = 'lcqmc'\n",
    "# Set the path to save the trained model\n",
    "MODEL_SAVE_PATH = f'../assets/{DATASET_NAME}-{MODEL_NAME}'\n",
    "# Set the rationale length ratio which determines the length of the extracted rationales.\n",
    "RATIONALE_RATIO = 0.7\n",
    "\n",
    "\n",
    "# Init model and tokenizer\n",
    "model = ErnieForSequenceClassification.from_pretrained(MODEL_NAME, num_classes=2)\n",
    "tokenizer = ErnieTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练\n",
    "这里以ERNIE-3.0为例训练一个文本相似度模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from paddlenlp.datasets import load_dataset\n",
    "from assets.utils import training_model\n",
    "\n",
    "# Load dataset\n",
    "train_ds, dev_ds, test_ds = load_dataset(DATASET_NAME, splits=[\"train\", \"dev\", \"test\"])\n",
    "\n",
    "# Start training\n",
    "training_model(model, tokenizer, train_ds, dev_ds, save_dir=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重要度分数获取\n",
    "该步为输入中每个词赋一个重要度分数，表示该词对预测的影响度。重要度分数获取共分三步。\n",
    "#### 1）加载模型和评测数据集\n",
    "更改模型以及评估数据的存储路径（MODEL_PATH和DATA_PATH），完成模型和评测数据集的加载。赛段一数据量为1712条，赛段二数据量为4167条，请确认评测数据集完整。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from assets.utils import load_data\n",
    "\n",
    "# Correct MODEL_PATH and DATA_PATH before executing\n",
    "MODEL_PATH = MODEL_SAVE_PATH + '/model_state.pdparams'\n",
    "DATA_PATH = '/home/aistudio/TrustAI/tutorials/assets/sim_interpretation_A.txt'\n",
    "\n",
    "# Load the trained parameters\n",
    "state_dict = paddle.load(MODEL_PATH)\n",
    "model.set_dict(state_dict)\n",
    "\n",
    "# Load test data\n",
    "data = load_data(DATA_PATH)\n",
    "print(\"Num of data:\", len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 2）数据预处理\n",
    "\n",
    "a) 输入格式化：将输入的两个文本组织成模型预测所需格式，如对于Ernie3.0-base模型，其输入形式为[CLS]query[SEP]title[SEP]\n",
    "\n",
    "b) 分词位置索引：计算每个分词结果对应的原文位置索引，这里的分词包括模型分词和标准分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T08:45:21.199708Z",
     "iopub.status.busy": "2022-06-29T08:45:21.199226Z",
     "iopub.status.idle": "2022-06-29T08:45:22.799968Z",
     "shell.execute_reply": "2022-06-29T08:45:22.799210Z",
     "shell.execute_reply.started": "2022-06-29T08:45:21.199667Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trustai.interpretation import get_word_offset\n",
    "\n",
    "# Add CLS and SEP tags to both original text and standard splited tokens\n",
    "contexts = []\n",
    "standard_split = []\n",
    "for idx in data:\n",
    "    example = data[idx]\n",
    "    contexts.append(\"[CLS]\" + example['query'] + \"[SEP]\" + example['title'] + \"[SEP]\")\n",
    "    standard_split.append([\"[CLS]\"] + example['text_q_seg'] + [\"[SEP]\"] + example['text_t_seg'] + [\"[SEP]\"])\n",
    "\n",
    "# Get the offset map of tokenized tokens and standard splited tokens\n",
    "ori_offset_maps = []\n",
    "standard_split_offset_maps = []\n",
    "for i in range(len(contexts)):\n",
    "    ori_offset_maps.append(tokenizer.get_offset_mapping(contexts[i]))\n",
    "    standard_split_offset_maps.append(get_word_offset(contexts[i], standard_split[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3）重要度分数获取\n",
    "我们提供attention，IG，LIME等三种解释方法，可根据实际实验结果选取最有效的一种方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a） Attention-based Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T08:46:21.198173Z",
     "iopub.status.busy": "2022-06-29T08:46:21.197258Z",
     "iopub.status.idle": "2022-06-29T08:46:35.417659Z",
     "shell.execute_reply": "2022-06-29T08:46:35.416892Z",
     "shell.execute_reply.started": "2022-06-29T08:46:21.198133Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trustai.interpretation.token_level.common import attention_predict_fn_on_paddlenlp\n",
    "from trustai.interpretation.token_level import AttentionInterpreter\n",
    "from assets.utils import create_dataloader_from_scratch\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Init an attention interpreter and get the importance scores\n",
    "att = AttentionInterpreter(model, device=\"gpu\", predict_fn=attention_predict_fn_on_paddlenlp)\n",
    "\n",
    "# Use attention interpreter to get the importance scores for all data\n",
    "interp_results = None\n",
    "for batch in create_dataloader_from_scratch(list(data.values()), tokenizer, BATCH_SIZE):\n",
    "    if interp_results:\n",
    "        interp_results += att(batch)\n",
    "    else:\n",
    "        interp_results = att(batch)\n",
    "\n",
    "# Align the results back to the standard splited tokens so that it can be evaluated correctly later\n",
    "align_res = att.alignment(interp_results, contexts, standard_split, standard_split_offset_maps, ori_offset_maps, special_tokens=[\"[CLS]\", '[SEP]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b）IG-based Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T08:48:39.900787Z",
     "iopub.status.busy": "2022-06-29T08:48:39.899882Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trustai.interpretation.token_level import IntGradInterpreter\n",
    "from assets.utils import create_dataloader_from_scratch\n",
    "# Hyperparameters\n",
    "IG_STEP = 100\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Init an IG interpreter\n",
    "ig = IntGradInterpreter(model, device=\"gpu\")\n",
    "\n",
    "# Use IG interpreter to get the importance scores for all data\n",
    "interp_results = None\n",
    "for batch in create_dataloader_from_scratch(list(data.values()), tokenizer, BATCH_SIZE):\n",
    "    if interp_results:\n",
    "        interp_results += ig(batch, steps=IG_STEP)\n",
    "    else:\n",
    "        interp_results = ig(batch, steps=IG_STEP)\n",
    "\n",
    "# Align the results back to the standard splited tokens so that it can be evaluated correctly later\n",
    "align_res = ig.alignment(interp_results, contexts, standard_split, standard_split_offset_maps, ori_offset_maps, special_tokens=[\"[CLS]\", '[SEP]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### c）LIME Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T04:59:33.472688Z",
     "iopub.status.busy": "2022-06-28T04:59:33.471876Z",
     "iopub.status.idle": "2022-06-28T05:51:45.367172Z",
     "shell.execute_reply": "2022-06-28T05:51:45.365805Z",
     "shell.execute_reply.started": "2022-06-28T04:59:33.472657Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from trustai.interpretation.token_level import LIMEInterpreter\n",
    "from assets.utils import create_dataloader_from_scratch\n",
    "# Hyperparameters\n",
    "LIME_SAMPLES = 1000\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "# Init an LIME interpreter\n",
    "lime = LIMEInterpreter(model, device=\"gpu\",\n",
    "    unk_id=tokenizer.convert_tokens_to_ids('[UNK]'),\n",
    "    pad_id=tokenizer.convert_tokens_to_ids('[PAD]'))\n",
    "\n",
    "# Use LIME interpreter to get the importance scores for all data\n",
    "interp_results = None\n",
    "for batch in create_dataloader_from_scratch(list(data.values()), tokenizer, BATCH_SIZE):\n",
    "    if interp_results:\n",
    "        interp_results += lime(batch, num_samples=LIME_SAMPLES)\n",
    "    else:\n",
    "        interp_results = lime(batch, num_samples=LIME_SAMPLES)\n",
    "    \n",
    "# Align the results back to the standard splited tokens so that it can be evaluated correctly later\n",
    "align_res = lime.alignment(interp_results, contexts, standard_split, standard_split_offset_maps, ori_offset_maps, special_tokens=[\"[CLS]\", '[SEP]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成用于评估的数据\n",
    "评估文件格式要求是4列数据：编号\\t预测标签\\t证据1\\t证据2，我们提供了脚本将模型输出结果转成评估所需格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-29T08:47:06.376052Z",
     "iopub.status.busy": "2022-06-29T08:47:06.375286Z",
     "iopub.status.idle": "2022-06-29T08:47:06.543470Z",
     "shell.execute_reply": "2022-06-29T08:47:06.542791Z",
     "shell.execute_reply.started": "2022-06-29T08:47:06.376011Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Re-sort the token index according to their importance scores\n",
    "def resort(index_array, importance_score):\n",
    "    res = sorted([[idx, importance_score[idx]] for idx in index_array], key=lambda x:x[1], reverse=True)\n",
    "    res = [n[0] for n in res]\n",
    "    return res\n",
    "\n",
    "# Post-prepare the result data so that it can be used for the evaluation directly\n",
    "def prepare_eval_data(data, results, paddle_model):\n",
    "    res = {}\n",
    "    for data_id, inter_res in zip(data, results):\n",
    "        # Split importance score vectors for query and title from inter_res.word_attributions\n",
    "        query_importance_score = np.array(inter_res.word_attributions[1:len(data[data_id]['text_q_seg'])+1])\n",
    "        title_importance_score = np.array(inter_res.word_attributions[len(data[data_id]['text_q_seg'])+2:-1])\n",
    "        # Extract topK importance scores\n",
    "        query_topk = math.ceil(len(data[data_id]['text_q_seg'])*RATIONALE_RATIO)\n",
    "        title_topk = math.ceil(len(data[data_id]['text_t_seg'])*RATIONALE_RATIO)\n",
    "        \n",
    "        eval_data = {}        \n",
    "        eval_data['id'] = data_id\n",
    "        eval_data['pred_label'] = inter_res.pred_label\n",
    "        # Find the token index of the topK importance scores\n",
    "        eval_data['rationale_q'] = np.argpartition(query_importance_score, -query_topk)[-query_topk:]\n",
    "        eval_data['rationale_t'] = np.argpartition(title_importance_score, -title_topk)[-title_topk:]\n",
    "        # Re-sort the token index according to their importance scores\n",
    "        eval_data['rationale_q'] = resort(eval_data['rationale_q'], query_importance_score)\n",
    "        eval_data['rationale_t'] = resort(eval_data['rationale_t'], title_importance_score)\n",
    "\n",
    "        res[data_id] = eval_data\n",
    "    return res\n",
    "\n",
    "# Generate results for evaluation\n",
    "predicts = prepare_eval_data(data, align_res, model)\n",
    "out_file = open('./sim_rationale.txt', 'w')\n",
    "for key in predicts:\n",
    "    out_file.write(str(predicts[key]['id'])+'\\t'+ str(predicts[key]['pred_label'])+'\\t')\n",
    "    for idx in predicts[key]['rationale_q'][:-1]:\n",
    "        out_file.write(str(idx)+',')\n",
    "    out_file.write(str(predicts[key]['rationale_q'][-1])+'\\t')\n",
    "\n",
    "    for idx in predicts[key]['rationale_t'][:-1]:\n",
    "        out_file.write(str(idx)+',')\n",
    "    out_file.write(str(predicts[key]['rationale_t'][-1])+'\\n')\n",
    "out_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
