{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 『2022信通院兴智杯：深度学习模型可解释竞赛』- 文本相似度可解释性评测\n",
    "## 1、项目介绍\n",
    "深度学习模型在很多NLP任务上已经取得巨大成功，但其常被当作一个黑盒使用，内部预测机制对使用者是不透明的。这使得深度学习模型结果不被人信任，增加落地难度，尤其是在医疗、法律等特殊领域。同时，当模型出现效果不好或鲁棒性差等问题时，由于不了解其内部机制，导致很难对模型进行优化。近期，深度学习模型的可解释性被越来越多的人关注。但模型的可解释性评估还不够完善，本基线提供了文本相似度任务的评测数据和相关评测指标，旨在评估模型的可解释性。\n",
    "## 2、基线运行\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 依赖安装\n",
    "安装一些必须的依赖包。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install paddlepaddle-gpu\n",
    "!pip3 install -U paddlenlp==2.3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据准备\n",
    "#### 模型训练数据\n",
    "中文文本相似度任务中，我们使用LCQMC数据集进行模型训练，使用paddlenlp框架自动缓存。\n",
    "#### 下载预训练模型\n",
    "使用paddlenlp框架自动缓存模型文件。\n",
    "#### 其他数据下载\n",
    "暂无。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 一些初始化工作\n",
    "初始化工作包括了模型选择及加载、训练数据集选择、模型存储路径设定、抽取证据的长度占原文本长度的比例设定等。可按需更改。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T04:58:58.853178Z",
     "iopub.status.busy": "2022-06-28T04:58:58.852545Z",
     "iopub.status.idle": "2022-06-28T04:59:13.028590Z",
     "shell.execute_reply": "2022-06-28T04:59:13.027656Z",
     "shell.execute_reply.started": "2022-06-28T04:58:58.853138Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-06-28 12:59:01,224] [    INFO] - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_base_zh.pdparams and saved to /home/aistudio/.paddlenlp/models/ernie-3.0-base-zh\n",
      "[2022-06-28 12:59:01,226] [    INFO] - Downloading ernie_3.0_base_zh.pdparams from https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_base_zh.pdparams\n",
      "100%|██████████| 452M/452M [00:06<00:00, 73.4MB/s] \n",
      "W0628 12:59:07.819039   182 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.2, Runtime API Version: 10.1\n",
      "W0628 12:59:07.823046   182 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n",
      "[2022-06-28 12:59:12,908] [    INFO] - Downloading https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_base_zh_vocab.txt and saved to /home/aistudio/.paddlenlp/models/ernie-3.0-base-zh\n",
      "[2022-06-28 12:59:12,911] [    INFO] - Downloading ernie_3.0_base_zh_vocab.txt from https://bj.bcebos.com/paddlenlp/models/transformers/ernie_3.0/ernie_3.0_base_zh_vocab.txt\n",
      "100%|██████████| 182k/182k [00:00<00:00, 26.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import paddle\n",
    "import paddlenlp\n",
    "from paddlenlp.transformers import ErnieForSequenceClassification, ErnieTokenizer\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "MODEL_NAME = \"ernie-3.0-base-zh\" # choose from [\"ernie-1.0\", \"ernie-1.0-base-zh\", \"ernie-1.0-large-zh-cw\", \"ernie-2.0-base-zh\", \"ernie-2.0-large-zh\", \"ernie-3.0-xbase-zh\", \"ernie-3.0-base-zh\", \"ernie-3.0-medium-zh\", \"ernie-3.0-mini-zh\", \"ernie-3.0-micro-zh\", \"ernie-3.0-nano-zh\"]\n",
    "DATASET_NAME = 'lcqmc'\n",
    "MODEL_SAVE_PATH = f'../assets/{DATASET_NAME}-{MODEL_NAME}'\n",
    "RATIONALE_RATIO = 0.7\n",
    "\n",
    "\n",
    "# Init model and tokenizer\n",
    "model = ErnieForSequenceClassification.from_pretrained(MODEL_NAME, num_classes=2)\n",
    "tokenizer = ErnieTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型\n",
    "这里以ERNIE-3.0为例训练一个文本相似度模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2022-06-21T11:11:14.874491Z",
     "iopub.status.busy": "2022-06-21T11:11:14.873687Z",
     "iopub.status.idle": "2022-06-21T12:26:03.075194Z",
     "shell.execute_reply": "2022-06-21T12:26:03.074508Z",
     "shell.execute_reply.started": "2022-06-21T11:11:14.874454Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-06-21 19:11:14,877] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh.pdparams\n",
      "[2022-06-21 19:11:17,019] [    INFO] - Already cached /home/aistudio/.paddlenlp/models/ernie-3.0-base-zh/ernie_3.0_base_zh_vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset labels: ['0', '1']\n",
      "dataset examples:\n",
      "{'query': '喜欢打篮球的男生喜欢什么样的女生', 'title': '爱打篮球的男生喜欢什么样的女生', 'label': 1}\n",
      "{'query': '我手机丢了，我想换个手机', 'title': '我想买个新手机，求推荐', 'label': 1}\n",
      "{'query': '大家觉得她好看吗', 'title': '大家觉得跑男好看吗？', 'label': 0}\n",
      "{'query': '求秋色之空漫画全集', 'title': '求秋色之空全集漫画', 'label': 1}\n",
      "{'query': '晚上睡觉带着耳机听音乐有什么害处吗？', 'title': '孕妇可以戴耳机听音乐吗?', 'label': 0}\n",
      "Training Starts:\n",
      "global step 100, epoch: 1, batch: 100, loss: 0.57603, acc: 0.55437\n",
      "global step 200, epoch: 1, batch: 200, loss: 0.55437, acc: 0.63703\n",
      "global step 300, epoch: 1, batch: 300, loss: 0.31157, acc: 0.70583\n",
      "global step 400, epoch: 1, batch: 400, loss: 0.23469, acc: 0.74852\n",
      "global step 500, epoch: 1, batch: 500, loss: 0.24344, acc: 0.77219\n",
      "global step 600, epoch: 1, batch: 600, loss: 0.37564, acc: 0.78682\n",
      "global step 700, epoch: 1, batch: 700, loss: 0.44287, acc: 0.79746\n",
      "global step 800, epoch: 1, batch: 800, loss: 0.42520, acc: 0.80582\n",
      "global step 900, epoch: 1, batch: 900, loss: 0.37588, acc: 0.81118\n",
      "global step 1000, epoch: 1, batch: 1000, loss: 0.43927, acc: 0.81672\n",
      "global step 1100, epoch: 1, batch: 1100, loss: 0.23508, acc: 0.82176\n",
      "global step 1200, epoch: 1, batch: 1200, loss: 0.35403, acc: 0.82503\n",
      "global step 1300, epoch: 1, batch: 1300, loss: 0.08674, acc: 0.82774\n",
      "global step 1400, epoch: 1, batch: 1400, loss: 0.32003, acc: 0.83089\n",
      "global step 1500, epoch: 1, batch: 1500, loss: 0.26941, acc: 0.83304\n",
      "global step 1600, epoch: 1, batch: 1600, loss: 0.34591, acc: 0.83547\n",
      "global step 1700, epoch: 1, batch: 1700, loss: 0.14514, acc: 0.83767\n",
      "global step 1800, epoch: 1, batch: 1800, loss: 0.32333, acc: 0.83845\n",
      "global step 1900, epoch: 1, batch: 1900, loss: 0.29054, acc: 0.84033\n",
      "global step 2000, epoch: 1, batch: 2000, loss: 0.30007, acc: 0.84195\n",
      "global step 2100, epoch: 1, batch: 2100, loss: 0.54918, acc: 0.84321\n",
      "global step 2200, epoch: 1, batch: 2200, loss: 0.46188, acc: 0.84433\n",
      "global step 2300, epoch: 1, batch: 2300, loss: 0.25051, acc: 0.84535\n",
      "global step 2400, epoch: 1, batch: 2400, loss: 0.30281, acc: 0.84629\n",
      "global step 2500, epoch: 1, batch: 2500, loss: 0.20721, acc: 0.84777\n",
      "global step 2600, epoch: 1, batch: 2600, loss: 0.10061, acc: 0.84875\n",
      "global step 2700, epoch: 1, batch: 2700, loss: 0.26989, acc: 0.84947\n",
      "global step 2800, epoch: 1, batch: 2800, loss: 0.22033, acc: 0.85064\n",
      "global step 2900, epoch: 1, batch: 2900, loss: 0.21551, acc: 0.85128\n",
      "global step 3000, epoch: 1, batch: 3000, loss: 0.19675, acc: 0.85196\n",
      "global step 3100, epoch: 1, batch: 3100, loss: 0.14902, acc: 0.85276\n",
      "global step 3200, epoch: 1, batch: 3200, loss: 0.23184, acc: 0.85336\n",
      "global step 3300, epoch: 1, batch: 3300, loss: 0.22160, acc: 0.85396\n",
      "global step 3400, epoch: 1, batch: 3400, loss: 0.28588, acc: 0.85476\n",
      "global step 3500, epoch: 1, batch: 3500, loss: 0.25313, acc: 0.85565\n",
      "global step 3600, epoch: 1, batch: 3600, loss: 0.18282, acc: 0.85663\n",
      "global step 3700, epoch: 1, batch: 3700, loss: 0.20610, acc: 0.85717\n",
      "global step 3800, epoch: 1, batch: 3800, loss: 0.19891, acc: 0.85799\n",
      "global step 3900, epoch: 1, batch: 3900, loss: 0.23297, acc: 0.85872\n",
      "global step 4000, epoch: 1, batch: 4000, loss: 0.16510, acc: 0.85923\n",
      "global step 4100, epoch: 1, batch: 4100, loss: 0.14989, acc: 0.85982\n",
      "global step 4200, epoch: 1, batch: 4200, loss: 0.29500, acc: 0.86022\n",
      "global step 4300, epoch: 1, batch: 4300, loss: 0.16427, acc: 0.86076\n",
      "global step 4400, epoch: 1, batch: 4400, loss: 0.38208, acc: 0.86129\n",
      "global step 4500, epoch: 1, batch: 4500, loss: 0.13081, acc: 0.86176\n",
      "global step 4600, epoch: 1, batch: 4600, loss: 0.17076, acc: 0.86230\n",
      "global step 4700, epoch: 1, batch: 4700, loss: 0.27863, acc: 0.86275\n",
      "global step 4800, epoch: 1, batch: 4800, loss: 0.33650, acc: 0.86296\n",
      "global step 4900, epoch: 1, batch: 4900, loss: 0.19613, acc: 0.86327\n",
      "global step 5000, epoch: 1, batch: 5000, loss: 0.13031, acc: 0.86346\n",
      "global step 5100, epoch: 1, batch: 5100, loss: 0.64539, acc: 0.86377\n",
      "global step 5200, epoch: 1, batch: 5200, loss: 0.28351, acc: 0.86405\n",
      "global step 5300, epoch: 1, batch: 5300, loss: 0.14910, acc: 0.86374\n",
      "global step 5400, epoch: 1, batch: 5400, loss: 0.27075, acc: 0.86432\n",
      "global step 5500, epoch: 1, batch: 5500, loss: 0.13223, acc: 0.86454\n",
      "global step 5600, epoch: 1, batch: 5600, loss: 0.19150, acc: 0.86483\n",
      "global step 5700, epoch: 1, batch: 5700, loss: 0.37300, acc: 0.86523\n",
      "global step 5800, epoch: 1, batch: 5800, loss: 0.18979, acc: 0.86550\n",
      "global step 5900, epoch: 1, batch: 5900, loss: 0.41851, acc: 0.86577\n",
      "global step 6000, epoch: 1, batch: 6000, loss: 0.21541, acc: 0.86605\n",
      "global step 6100, epoch: 1, batch: 6100, loss: 0.13812, acc: 0.86622\n",
      "global step 6200, epoch: 1, batch: 6200, loss: 0.39427, acc: 0.86620\n",
      "global step 6300, epoch: 1, batch: 6300, loss: 0.27133, acc: 0.86624\n",
      "global step 6400, epoch: 1, batch: 6400, loss: 0.47945, acc: 0.86647\n",
      "global step 6500, epoch: 1, batch: 6500, loss: 0.26883, acc: 0.86667\n",
      "global step 6600, epoch: 1, batch: 6600, loss: 0.25037, acc: 0.86698\n",
      "global step 6700, epoch: 1, batch: 6700, loss: 0.19742, acc: 0.86734\n",
      "global step 6800, epoch: 1, batch: 6800, loss: 0.23522, acc: 0.86770\n",
      "global step 6900, epoch: 1, batch: 6900, loss: 0.16355, acc: 0.86791\n",
      "global step 7000, epoch: 1, batch: 7000, loss: 0.29337, acc: 0.86812\n",
      "global step 7100, epoch: 1, batch: 7100, loss: 0.27554, acc: 0.86823\n",
      "global step 7200, epoch: 1, batch: 7200, loss: 0.25136, acc: 0.86852\n",
      "global step 7300, epoch: 1, batch: 7300, loss: 0.41819, acc: 0.86870\n",
      "global step 7400, epoch: 1, batch: 7400, loss: 0.24573, acc: 0.86880\n",
      "eval loss: 0.28014, accu: 0.89071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-06-21 19:36:33,288] [    INFO] - tokenizer config file saved in ../assets/lcqmc-ernie-3.0-base-zh/tokenizer_config.json\n",
      "[2022-06-21 19:36:33,291] [    INFO] - Special tokens file saved in ../assets/lcqmc-ernie-3.0-base-zh/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 7500, epoch: 2, batch: 38, loss: 0.24792, acc: 0.89720\n",
      "global step 7600, epoch: 2, batch: 138, loss: 0.05826, acc: 0.89855\n",
      "global step 7700, epoch: 2, batch: 238, loss: 0.28852, acc: 0.89916\n",
      "global step 7800, epoch: 2, batch: 338, loss: 0.26902, acc: 0.89858\n",
      "global step 7900, epoch: 2, batch: 438, loss: 0.20164, acc: 0.89947\n",
      "global step 8000, epoch: 2, batch: 538, loss: 0.29403, acc: 0.90027\n",
      "global step 8100, epoch: 2, batch: 638, loss: 0.12226, acc: 0.90125\n",
      "global step 8200, epoch: 2, batch: 738, loss: 0.18106, acc: 0.90058\n",
      "global step 8300, epoch: 2, batch: 838, loss: 0.12764, acc: 0.89898\n",
      "global step 8400, epoch: 2, batch: 938, loss: 0.13077, acc: 0.89829\n",
      "global step 8500, epoch: 2, batch: 1038, loss: 0.25547, acc: 0.89800\n",
      "global step 8600, epoch: 2, batch: 1138, loss: 0.19892, acc: 0.89859\n",
      "global step 8700, epoch: 2, batch: 1238, loss: 0.41702, acc: 0.89812\n",
      "global step 8800, epoch: 2, batch: 1338, loss: 0.18125, acc: 0.89822\n",
      "global step 8900, epoch: 2, batch: 1438, loss: 0.18356, acc: 0.89773\n",
      "global step 9000, epoch: 2, batch: 1538, loss: 0.36195, acc: 0.89688\n",
      "global step 9100, epoch: 2, batch: 1638, loss: 0.08362, acc: 0.89725\n",
      "global step 9200, epoch: 2, batch: 1738, loss: 0.16649, acc: 0.89755\n",
      "global step 9300, epoch: 2, batch: 1838, loss: 0.11776, acc: 0.89746\n",
      "global step 9400, epoch: 2, batch: 1938, loss: 0.17023, acc: 0.89727\n",
      "global step 9500, epoch: 2, batch: 2038, loss: 0.15695, acc: 0.89794\n",
      "global step 9600, epoch: 2, batch: 2138, loss: 0.14758, acc: 0.89806\n",
      "global step 9700, epoch: 2, batch: 2238, loss: 0.18708, acc: 0.89821\n",
      "global step 9800, epoch: 2, batch: 2338, loss: 0.17752, acc: 0.89814\n",
      "global step 9900, epoch: 2, batch: 2438, loss: 0.19545, acc: 0.89824\n",
      "global step 10000, epoch: 2, batch: 2538, loss: 0.27052, acc: 0.89789\n",
      "global step 10100, epoch: 2, batch: 2638, loss: 0.20682, acc: 0.89789\n",
      "global step 10200, epoch: 2, batch: 2738, loss: 0.25294, acc: 0.89763\n",
      "global step 10300, epoch: 2, batch: 2838, loss: 0.42710, acc: 0.89764\n",
      "global step 10400, epoch: 2, batch: 2938, loss: 0.12359, acc: 0.89771\n",
      "global step 10500, epoch: 2, batch: 3038, loss: 0.12331, acc: 0.89762\n",
      "global step 10600, epoch: 2, batch: 3138, loss: 0.28991, acc: 0.89747\n",
      "global step 10700, epoch: 2, batch: 3238, loss: 0.18298, acc: 0.89757\n",
      "global step 10800, epoch: 2, batch: 3338, loss: 0.14921, acc: 0.89750\n",
      "global step 10900, epoch: 2, batch: 3438, loss: 0.25664, acc: 0.89729\n",
      "global step 11000, epoch: 2, batch: 3538, loss: 0.31166, acc: 0.89721\n",
      "global step 11100, epoch: 2, batch: 3638, loss: 0.19730, acc: 0.89729\n",
      "global step 11200, epoch: 2, batch: 3738, loss: 0.23220, acc: 0.89723\n",
      "global step 11300, epoch: 2, batch: 3838, loss: 0.16072, acc: 0.89745\n",
      "global step 11400, epoch: 2, batch: 3938, loss: 0.21628, acc: 0.89741\n",
      "global step 11500, epoch: 2, batch: 4038, loss: 0.49205, acc: 0.89753\n",
      "global step 11600, epoch: 2, batch: 4138, loss: 0.11396, acc: 0.89757\n",
      "global step 11700, epoch: 2, batch: 4238, loss: 0.17548, acc: 0.89725\n",
      "global step 11800, epoch: 2, batch: 4338, loss: 0.10363, acc: 0.89730\n",
      "global step 11900, epoch: 2, batch: 4438, loss: 0.16093, acc: 0.89758\n",
      "global step 12000, epoch: 2, batch: 4538, loss: 0.23650, acc: 0.89770\n",
      "global step 12100, epoch: 2, batch: 4638, loss: 0.12031, acc: 0.89752\n",
      "global step 12200, epoch: 2, batch: 4738, loss: 0.23270, acc: 0.89748\n",
      "global step 12300, epoch: 2, batch: 4838, loss: 0.35043, acc: 0.89751\n",
      "global step 12400, epoch: 2, batch: 4938, loss: 0.24407, acc: 0.89763\n",
      "global step 12500, epoch: 2, batch: 5038, loss: 0.26824, acc: 0.89768\n",
      "global step 12600, epoch: 2, batch: 5138, loss: 0.18821, acc: 0.89766\n",
      "global step 12700, epoch: 2, batch: 5238, loss: 0.16554, acc: 0.89775\n",
      "global step 12800, epoch: 2, batch: 5338, loss: 0.13950, acc: 0.89771\n",
      "global step 12900, epoch: 2, batch: 5438, loss: 0.12661, acc: 0.89775\n",
      "global step 13000, epoch: 2, batch: 5538, loss: 0.21612, acc: 0.89761\n",
      "global step 13100, epoch: 2, batch: 5638, loss: 0.26581, acc: 0.89733\n",
      "global step 13200, epoch: 2, batch: 5738, loss: 0.15093, acc: 0.89695\n",
      "global step 13300, epoch: 2, batch: 5838, loss: 0.33674, acc: 0.89686\n",
      "global step 13400, epoch: 2, batch: 5938, loss: 0.20783, acc: 0.89668\n",
      "global step 13500, epoch: 2, batch: 6038, loss: 0.20158, acc: 0.89672\n",
      "global step 13600, epoch: 2, batch: 6138, loss: 0.26977, acc: 0.89673\n",
      "global step 13700, epoch: 2, batch: 6238, loss: 0.17792, acc: 0.89667\n",
      "global step 13800, epoch: 2, batch: 6338, loss: 0.24431, acc: 0.89677\n",
      "global step 13900, epoch: 2, batch: 6438, loss: 0.23546, acc: 0.89679\n",
      "global step 14000, epoch: 2, batch: 6538, loss: 0.13899, acc: 0.89686\n",
      "global step 14100, epoch: 2, batch: 6638, loss: 0.34578, acc: 0.89697\n",
      "global step 14200, epoch: 2, batch: 6738, loss: 0.12041, acc: 0.89695\n",
      "global step 14300, epoch: 2, batch: 6838, loss: 0.15571, acc: 0.89699\n",
      "global step 14400, epoch: 2, batch: 6938, loss: 0.25660, acc: 0.89694\n",
      "global step 14500, epoch: 2, batch: 7038, loss: 0.34441, acc: 0.89700\n",
      "global step 14600, epoch: 2, batch: 7138, loss: 0.15054, acc: 0.89707\n",
      "global step 14700, epoch: 2, batch: 7238, loss: 0.29427, acc: 0.89708\n",
      "global step 14800, epoch: 2, batch: 7338, loss: 0.16392, acc: 0.89703\n",
      "global step 14900, epoch: 2, batch: 7438, loss: 0.25701, acc: 0.89709\n",
      "eval loss: 0.27927, accu: 0.89786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-06-21 20:01:19,573] [    INFO] - tokenizer config file saved in ../assets/lcqmc-ernie-3.0-base-zh/tokenizer_config.json\n",
      "[2022-06-21 20:01:19,575] [    INFO] - Special tokens file saved in ../assets/lcqmc-ernie-3.0-base-zh/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global step 15000, epoch: 3, batch: 76, loss: 0.23220, acc: 0.91118\n",
      "global step 15100, epoch: 3, batch: 176, loss: 0.07116, acc: 0.91033\n",
      "global step 15200, epoch: 3, batch: 276, loss: 0.12253, acc: 0.91214\n",
      "global step 15300, epoch: 3, batch: 376, loss: 0.33100, acc: 0.91240\n",
      "global step 15400, epoch: 3, batch: 476, loss: 0.18419, acc: 0.91170\n",
      "global step 15500, epoch: 3, batch: 576, loss: 0.31150, acc: 0.91151\n",
      "global step 15600, epoch: 3, batch: 676, loss: 0.07493, acc: 0.91207\n",
      "global step 15700, epoch: 3, batch: 776, loss: 0.13670, acc: 0.91096\n",
      "global step 15800, epoch: 3, batch: 876, loss: 0.22541, acc: 0.91067\n",
      "global step 15900, epoch: 3, batch: 976, loss: 0.18129, acc: 0.91028\n",
      "global step 16000, epoch: 3, batch: 1076, loss: 0.20111, acc: 0.91081\n",
      "global step 16100, epoch: 3, batch: 1176, loss: 0.19327, acc: 0.91013\n",
      "global step 16200, epoch: 3, batch: 1276, loss: 0.18196, acc: 0.91022\n",
      "global step 16300, epoch: 3, batch: 1376, loss: 0.20761, acc: 0.91022\n",
      "global step 16400, epoch: 3, batch: 1476, loss: 0.15965, acc: 0.91057\n",
      "global step 16500, epoch: 3, batch: 1576, loss: 0.42328, acc: 0.91113\n",
      "global step 16600, epoch: 3, batch: 1676, loss: 0.06357, acc: 0.91132\n",
      "global step 16700, epoch: 3, batch: 1776, loss: 0.18783, acc: 0.91156\n",
      "global step 16800, epoch: 3, batch: 1876, loss: 0.10561, acc: 0.91171\n",
      "global step 16900, epoch: 3, batch: 1976, loss: 0.21341, acc: 0.91156\n",
      "global step 17000, epoch: 3, batch: 2076, loss: 0.09178, acc: 0.91096\n",
      "global step 17100, epoch: 3, batch: 2176, loss: 0.16837, acc: 0.91125\n",
      "global step 17200, epoch: 3, batch: 2276, loss: 0.07770, acc: 0.91159\n",
      "global step 17300, epoch: 3, batch: 2376, loss: 0.30165, acc: 0.91197\n",
      "global step 17400, epoch: 3, batch: 2476, loss: 0.16049, acc: 0.91171\n",
      "global step 17500, epoch: 3, batch: 2576, loss: 0.39668, acc: 0.91188\n",
      "global step 17600, epoch: 3, batch: 2676, loss: 0.10852, acc: 0.91160\n",
      "global step 17700, epoch: 3, batch: 2776, loss: 0.14347, acc: 0.91188\n",
      "global step 17800, epoch: 3, batch: 2876, loss: 0.18433, acc: 0.91167\n",
      "global step 17900, epoch: 3, batch: 2976, loss: 0.20476, acc: 0.91203\n",
      "global step 18000, epoch: 3, batch: 3076, loss: 0.06521, acc: 0.91205\n",
      "global step 18100, epoch: 3, batch: 3176, loss: 0.16955, acc: 0.91203\n",
      "global step 18200, epoch: 3, batch: 3276, loss: 0.25567, acc: 0.91210\n",
      "global step 18300, epoch: 3, batch: 3376, loss: 0.06130, acc: 0.91205\n",
      "global step 18400, epoch: 3, batch: 3476, loss: 0.18515, acc: 0.91193\n",
      "global step 18500, epoch: 3, batch: 3576, loss: 0.26041, acc: 0.91186\n",
      "global step 18600, epoch: 3, batch: 3676, loss: 0.14072, acc: 0.91196\n",
      "global step 18700, epoch: 3, batch: 3776, loss: 0.17386, acc: 0.91189\n",
      "global step 18800, epoch: 3, batch: 3876, loss: 0.14941, acc: 0.91194\n",
      "global step 18900, epoch: 3, batch: 3976, loss: 0.14687, acc: 0.91185\n",
      "global step 19000, epoch: 3, batch: 4076, loss: 0.08697, acc: 0.91199\n",
      "global step 19100, epoch: 3, batch: 4176, loss: 0.12886, acc: 0.91209\n",
      "global step 19200, epoch: 3, batch: 4276, loss: 0.32809, acc: 0.91210\n",
      "global step 19300, epoch: 3, batch: 4376, loss: 0.16349, acc: 0.91200\n",
      "global step 19400, epoch: 3, batch: 4476, loss: 0.13897, acc: 0.91207\n",
      "global step 19500, epoch: 3, batch: 4576, loss: 0.14135, acc: 0.91220\n",
      "global step 19600, epoch: 3, batch: 4676, loss: 0.17366, acc: 0.91219\n",
      "global step 19700, epoch: 3, batch: 4776, loss: 0.13159, acc: 0.91231\n",
      "global step 19800, epoch: 3, batch: 4876, loss: 0.17217, acc: 0.91250\n",
      "global step 19900, epoch: 3, batch: 4976, loss: 0.31153, acc: 0.91253\n",
      "global step 20000, epoch: 3, batch: 5076, loss: 0.17380, acc: 0.91247\n",
      "global step 20100, epoch: 3, batch: 5176, loss: 0.15902, acc: 0.91261\n",
      "global step 20200, epoch: 3, batch: 5276, loss: 0.04504, acc: 0.91269\n",
      "global step 20300, epoch: 3, batch: 5376, loss: 0.15132, acc: 0.91278\n",
      "global step 20400, epoch: 3, batch: 5476, loss: 0.26207, acc: 0.91271\n",
      "global step 20500, epoch: 3, batch: 5576, loss: 0.25193, acc: 0.91266\n",
      "global step 20600, epoch: 3, batch: 5676, loss: 0.16734, acc: 0.91269\n",
      "global step 20700, epoch: 3, batch: 5776, loss: 0.10317, acc: 0.91272\n",
      "global step 20800, epoch: 3, batch: 5876, loss: 0.13031, acc: 0.91263\n",
      "global step 20900, epoch: 3, batch: 5976, loss: 0.13544, acc: 0.91274\n",
      "global step 21000, epoch: 3, batch: 6076, loss: 0.25921, acc: 0.91274\n",
      "global step 21100, epoch: 3, batch: 6176, loss: 0.23747, acc: 0.91271\n",
      "global step 21200, epoch: 3, batch: 6276, loss: 0.02540, acc: 0.91276\n",
      "global step 21300, epoch: 3, batch: 6376, loss: 0.29014, acc: 0.91270\n",
      "global step 21400, epoch: 3, batch: 6476, loss: 0.06264, acc: 0.91276\n",
      "global step 21500, epoch: 3, batch: 6576, loss: 0.11894, acc: 0.91275\n",
      "global step 21600, epoch: 3, batch: 6676, loss: 0.18950, acc: 0.91278\n",
      "global step 21700, epoch: 3, batch: 6776, loss: 0.16213, acc: 0.91278\n",
      "global step 21800, epoch: 3, batch: 6876, loss: 0.14991, acc: 0.91275\n",
      "global step 21900, epoch: 3, batch: 6976, loss: 0.13522, acc: 0.91274\n",
      "global step 22000, epoch: 3, batch: 7076, loss: 0.11584, acc: 0.91267\n",
      "global step 22100, epoch: 3, batch: 7176, loss: 0.07514, acc: 0.91272\n",
      "global step 22200, epoch: 3, batch: 7276, loss: 0.11783, acc: 0.91272\n",
      "global step 22300, epoch: 3, batch: 7376, loss: 0.20386, acc: 0.91262\n",
      "eval loss: 0.28017, accu: 0.90298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-06-21 20:26:03,057] [    INFO] - tokenizer config file saved in ../assets/lcqmc-ernie-3.0-base-zh/tokenizer_config.json\n",
      "[2022-06-21 20:26:03,060] [    INFO] - Special tokens file saved in ../assets/lcqmc-ernie-3.0-base-zh/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best accuracy is 0.902977!\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.datasets import load_dataset\n",
    "from assets.utils import training_model\n",
    "\n",
    "# Load dataset\n",
    "train_ds, dev_ds, test_ds = load_dataset(DATASET_NAME, splits=[\"train\", \"dev\", \"test\"])\n",
    "\n",
    "# Start training\n",
    "training_model(model, tokenizer, train_ds, dev_ds, save_dir=MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 重要度分数获取\n",
    "从这一步开始，我们进行对训练好的模型的可解释分析。首先获取评测数据集上数据的重要性分数。这里我们先更改一下训练好的模型以及评估数据的存储路径（MODEL_PATH和DATA_PATH），加载训练好的模型以及评测数据集，然后做一些数据上的预处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T06:57:41.043530Z",
     "iopub.status.busy": "2022-06-28T06:57:41.043103Z",
     "iopub.status.idle": "2022-06-28T06:57:43.694034Z",
     "shell.execute_reply": "2022-06-28T06:57:43.693300Z",
     "shell.execute_reply.started": "2022-06-28T06:57:41.043501Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of data: 1712\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from trustai.interpretation.token_level import IntGradInterpreter\n",
    "from assets.utils import convert_example, load_data\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "from trustai.interpretation import get_word_offset\n",
    "from assets.utils import predict, load_data\n",
    "\n",
    "# Correct MODEL_PATH and DATA_PATH before executing\n",
    "MODEL_PATH = MODEL_SAVE_PATH + '/model_state.pdparams'\n",
    "DATA_PATH = '/home/aistudio/TrustAI/tutorials/assets/sim_interpretation_A.txt'\n",
    "\n",
    "# Function to process data\n",
    "def preprocess_fn(data):\n",
    "    examples = []\n",
    "\n",
    "    if isinstance(data, dict):\n",
    "        data_t = []\n",
    "        for d in data:\n",
    "            data_t.append(data[d])\n",
    "        data = data_t\n",
    "\n",
    "    for text in data:\n",
    "        input_ids, segment_ids = convert_example(text, tokenizer, max_seq_length=128, is_test=True)\n",
    "        examples.append((input_ids, segment_ids))\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input id\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # segment id\n",
    "    ): fn(samples)\n",
    "\n",
    "    input_ids, segment_ids = batchify_fn(examples)\n",
    "    return paddle.to_tensor(input_ids, stop_gradient=False), paddle.to_tensor(segment_ids, stop_gradient=False)\n",
    "\n",
    "# Load the trained parameters\n",
    "state_dict = paddle.load(MODEL_PATH)\n",
    "model.set_dict(state_dict)\n",
    "\n",
    "# Prepare test data\n",
    "data = load_data(DATA_PATH)\n",
    "print(\"Num of data:\", len(data))\n",
    "\n",
    "# Get the combined contexts of both original text and standard splited text\n",
    "contexts = []\n",
    "standard_split = []\n",
    "for idx in data:\n",
    "    example = data[idx]\n",
    "    contexts.append(\"[CLS]\" + example['query'] + \"[SEP]\" + example['title'] + \"[SEP]\")\n",
    "    standard_split.append([\"[CLS]\"] + example['text_q_seg'] + [\"[SEP]\"] + example['text_t_seg'] + [\"[SEP]\"])\n",
    "\n",
    "# Get the offset map of tokenized tokens and standard splited tokens\n",
    "ori_offset_maps = []\n",
    "standard_split_offset_maps = []\n",
    "for i in range(len(contexts)):\n",
    "    ori_offset_maps.append(tokenizer.get_offset_mapping(contexts[i]))\n",
    "    standard_split_offset_maps.append(get_word_offset(contexts[i], standard_split[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们可以选取任何一种解释方法，去获取重要性分数。这里提供attention，IG，以及LIME这三种作为示例。实际使用时选择其中一种运行即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Interpreter获取重要性分数\n",
    "用attention获取评测数据集上数据的重要性分数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T06:59:35.701020Z",
     "iopub.status.busy": "2022-06-24T06:59:35.700533Z",
     "iopub.status.idle": "2022-06-24T06:59:48.486289Z",
     "shell.execute_reply": "2022-06-24T06:59:48.485508Z",
     "shell.execute_reply.started": "2022-06-24T06:59:35.700978Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trustai.interpretation.token_level.common import attention_predict_fn_on_paddlenlp\n",
    "from trustai.interpretation.token_level import AttentionInterpreter\n",
    "from assets.utils import create_dataloader_from_scratch\n",
    "\n",
    "# Init an attention interpreter and get the importance scores\n",
    "att = AttentionInterpreter(model, device=\"gpu\", predict_fn=attention_predict_fn_on_paddlenlp)\n",
    "\n",
    "# Use attention interpreter to get the importance scores for all data\n",
    "interp_results = None\n",
    "for batch in create_dataloader_from_scratch(list(data.values()), tokenizer, 8):\n",
    "    if interp_results:\n",
    "        interp_results += att(batch)\n",
    "    else:\n",
    "        interp_results = att(batch)\n",
    "\n",
    "# Align the results back to the standard splited tokens so that it can be evaluated correctly later\n",
    "align_res = att.alignment(interp_results, contexts, standard_split, standard_split_offset_maps, ori_offset_maps, special_tokens=[\"[CLS]\", '[SEP]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IG Interpreter获取重要性分数\n",
    "用IG的方法获取评测数据集上数据的重要性分数，这一步会消耗相对长的时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-24T07:52:29.778615Z",
     "iopub.status.busy": "2022-06-24T07:52:29.778127Z",
     "iopub.status.idle": "2022-06-24T08:16:12.404300Z",
     "shell.execute_reply": "2022-06-24T08:16:12.403483Z",
     "shell.execute_reply.started": "2022-06-24T07:52:29.778573Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from trustai.interpretation.token_level import IntGradInterpreter\n",
    "from assets.utils import create_dataloader_from_scratch\n",
    "# Hyperparameters\n",
    "IG_STEP = 100\n",
    "\n",
    "# Init an IG interpreter\n",
    "ig = IntGradInterpreter(model, device=\"gpu\")\n",
    "\n",
    "# Use IG interpreter to get the importance scores for all data\n",
    "interp_results = None\n",
    "for batch in create_dataloader_from_scratch(list(data.values()), tokenizer, 8):\n",
    "    if interp_results:\n",
    "        interp_results += ig(batch, steps=IG_STEP)\n",
    "    else:\n",
    "        interp_results = ig(batch, steps=IG_STEP)\n",
    "\n",
    "# Align the results back to the standard splited tokens so that it can be evaluated correctly later\n",
    "align_res = ig.alignment(interp_results, contexts, standard_split, standard_split_offset_maps, ori_offset_maps, special_tokens=[\"[CLS]\", '[SEP]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LIME Interpreter获取重要性分数\n",
    "用LIME的方法获取评测数据集上数据的重要性分数，这一步会消耗相对长的时间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T04:59:33.472688Z",
     "iopub.status.busy": "2022-06-28T04:59:33.471876Z",
     "iopub.status.idle": "2022-06-28T05:51:45.367172Z",
     "shell.execute_reply": "2022-06-28T05:51:45.365805Z",
     "shell.execute_reply.started": "2022-06-28T04:59:33.472657Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from trustai.interpretation.token_level import LIMEInterpreter\n",
    "from assets.utils import create_dataloader_from_scratch\n",
    "# Hyperparameters\n",
    "LIME_SAMPLES = 1000\n",
    "\n",
    "# Init an LIME interpreter\n",
    "lime = LIMEInterpreter(model, device=\"gpu\",\n",
    "    unk_id=tokenizer.convert_tokens_to_ids('[UNK]'),\n",
    "    pad_id=tokenizer.convert_tokens_to_ids('[PAD]'))\n",
    "\n",
    "# Use LIME interpreter to get the importance scores for all data\n",
    "interp_results = None\n",
    "for batch in create_dataloader_from_scratch(list(data.values()), tokenizer, 8):\n",
    "    if interp_results:\n",
    "        interp_results += lime(batch, num_samples=LIME_SAMPLES)\n",
    "    else:\n",
    "        interp_results = lime(batch, num_samples=LIME_SAMPLES)\n",
    "    \n",
    "# Align the results back to the standard splited tokens so that it can be evaluated correctly later\n",
    "align_res = lime.alignment(interp_results, contexts, standard_split, standard_split_offset_maps, ori_offset_maps, special_tokens=[\"[CLS]\", '[SEP]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成用于评估的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-06-28T05:53:13.574862Z",
     "iopub.status.busy": "2022-06-28T05:53:13.574340Z",
     "iopub.status.idle": "2022-06-28T05:53:13.743005Z",
     "shell.execute_reply": "2022-06-28T05:53:13.742307Z",
     "shell.execute_reply.started": "2022-06-28T05:53:13.574820Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Re-sort the token index according to their importance scores\n",
    "def resort(index_array, importance_score):\n",
    "    res = sorted([[idx, importance_score[idx]] for idx in index_array], key=lambda x:x[1], reverse=True)\n",
    "    res = [n[0] for n in res]\n",
    "    return res\n",
    "\n",
    "# Post-prepare the result data so that it can be used for the evaluation directly\n",
    "def prepare_eval_data(data, results, paddle_model):\n",
    "    res = {}\n",
    "    for data_id, inter_res in zip(data, results):\n",
    "        # Split importance score vectors for query and title from inter_res.word_attributions\n",
    "        query_importance_score = np.array(inter_res.word_attributions[1:len(data[data_id]['text_q_seg'])+1])\n",
    "        title_importance_score = np.array(inter_res.word_attributions[len(data[data_id]['text_q_seg'])+2:-1])\n",
    "        # Extract topK importance scores\n",
    "        query_topk = math.ceil(len(data[data_id]['text_q_seg'])*RATIONALE_RATIO)\n",
    "        title_topk = math.ceil(len(data[data_id]['text_t_seg'])*RATIONALE_RATIO)\n",
    "        \n",
    "        eval_data = {}        \n",
    "        eval_data['id'] = data_id\n",
    "        eval_data['pred_label'] = inter_res.pred_label\n",
    "        # Find the token index of the topK importance scores\n",
    "        eval_data['rationale_q'] = np.argpartition(query_importance_score, -query_topk)[-query_topk:]\n",
    "        eval_data['rationale_t'] = np.argpartition(title_importance_score, -title_topk)[-title_topk:]\n",
    "        # Re-sort the token index according to their importance scores\n",
    "        eval_data['rationale_q'] = resort(eval_data['rationale_q'], query_importance_score)\n",
    "        eval_data['rationale_t'] = resort(eval_data['rationale_t'], title_importance_score)\n",
    "\n",
    "        res[data_id] = eval_data\n",
    "    return res\n",
    "\n",
    "# Generate results for evaluation\n",
    "predicts = prepare_eval_data(data, align_res, model)\n",
    "out_file = open('./sim_rationale.txt', 'w')\n",
    "for key in predicts:\n",
    "    out_file.write(str(predicts[key]['id'])+'\\t'+ str(predicts[key]['pred_label'])+'\\t')\n",
    "    for idx in predicts[key]['rationale_q'][:-1]:\n",
    "        out_file.write(str(idx)+',')\n",
    "    out_file.write(str(predicts[key]['rationale_q'][-1])+'\\t')\n",
    "\n",
    "    for idx in predicts[key]['rationale_t'][:-1]:\n",
    "        out_file.write(str(idx)+',')\n",
    "    out_file.write(str(predicts[key]['rationale_t'][-1])+'\\n')\n",
    "out_file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
