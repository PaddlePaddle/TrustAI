{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application for similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(\"../../trustai/\")\n",
    "import paddle\n",
    "import paddlenlp\n",
    "from paddlenlp.transformers import ErnieForSequenceClassification, ErnieTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2022-04-26 11:01:14,070] [    INFO]\u001b[0m - Already cached /home/zhangshuai/.paddlenlp/models/ernie-1.0/ernie_v1_chn_base.pdparams\u001b[0m\n",
      "W0426 11:01:14.073176 14110 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 11.4, Runtime API Version: 10.2\n",
      "W0426 11:01:14.078840 14110 device_context.cc:465] device: 0, cuDNN Version: 8.2.\n",
      "\u001b[32m[2022-04-26 11:01:18,771] [    INFO]\u001b[0m - Already cached /home/zhangshuai/.paddlenlp/models/ernie-1.0/vocab.txt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"ernie-1.0\"\n",
    " \n",
    "model = ErnieForSequenceClassification.from_pretrained(MODEL_NAME, num_classes=2)\n",
    "tokenizer = ErnieTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load model paramerters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-04-26 11:01:19--  https://trustai.bj.bcebos.com/lcqmc-ernie-1.0.tar\n",
      "Resolving trustai.bj.bcebos.com (trustai.bj.bcebos.com)... 10.70.0.165\n",
      "Connecting to trustai.bj.bcebos.com (trustai.bj.bcebos.com)|10.70.0.165|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 399595520 (381M) [application/x-tar]\n",
      "Saving to: ‘lcqmc-ernie-1.0.tar’\n",
      "\n",
      "100%[======================================>] 399,595,520 94.1MB/s   in 4.0s   \n",
      "\n",
      "2022-04-26 11:01:23 (94.9 MB/s) - ‘lcqmc-ernie-1.0.tar’ saved [399595520/399595520]\n",
      "\n",
      "lcqmc-ernie-1.0/\n",
      "lcqmc-ernie-1.0/tokenizer_config.json\n",
      "lcqmc-ernie-1.0/vocab.txt\n",
      "lcqmc-ernie-1.0/model_state.pdparams\n",
      "lcqmc-ernie-1.0/model_config.json\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.datasets import load_dataset\n",
    "\n",
    "DATASET_NAME = 'lcqmc'\n",
    "train_ds, dev_ds, test_ds = load_dataset(DATASET_NAME, splits=[\"train\", \"dev\", \"test\"])\n",
    "\n",
    "# Load the trained model.\n",
    "!wget --no-check-certificate -c https://trustai.bj.bcebos.com/lcqmc-ernie-1.0.tar\n",
    "!tar -xvf ./lcqmc-ernie-1.0.tar -C ../assets/\n",
    "!rm ./lcqmc-ernie-1.0.tar\n",
    "\n",
    "state_dict = paddle.load(f'../assets/{DATASET_NAME}-{MODEL_NAME}/model_state.pdparams')\n",
    "model.set_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data size: 8802\n",
      "acc: 0.895251079300159\n"
     ]
    }
   ],
   "source": [
    "from assets.utils import predict\n",
    "\n",
    "label_map = {0 : 'negative', 1 : 'positive'}\n",
    "\n",
    "true_labels = [1, 1, 0]\n",
    "batch_size = 32\n",
    "predict_results = predict(model, dev_ds, tokenizer, label_map, batch_size=batch_size)\n",
    "\n",
    "count = 0\n",
    "right = 0\n",
    "for idx, example in enumerate(dev_ds):\n",
    "    count += 1\n",
    "    if label_map[example['label']] == predict_results[idx]:\n",
    "        right += 1\n",
    "print('data size:', count)\n",
    "print('acc:', right / count)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare for Interpretations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpretation.token_level import IntGradInterpreter\n",
    "import numpy as np\n",
    "from assets.utils import convert_example, load_data\n",
    "from paddlenlp.data import Stack, Tuple, Pad\n",
    "\n",
    "def preprocess_fn(data):\n",
    "    examples = []\n",
    "\n",
    "    if not isinstance(data, list):\n",
    "        data = [data]\n",
    "\n",
    "    for text in data:\n",
    "        input_ids, segment_ids = convert_example(text, tokenizer, max_seq_length=128, is_test=True)\n",
    "        examples.append((input_ids, segment_ids))\n",
    "\n",
    "    batchify_fn = lambda samples, fn=Tuple(\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # input id\n",
    "        Pad(axis=0, pad_val=tokenizer.pad_token_id),  # segment id\n",
    "    ): fn(samples)\n",
    "\n",
    "    input_ids, segment_ids = batchify_fn(examples)\n",
    "    return paddle.to_tensor(input_ids, stop_gradient=False), paddle.to_tensor(segment_ids, stop_gradient=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IG Interpreter\n",
    "This process will take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "from interpretation.token_level import IntGradInterpreter\n",
    "interp_results = []\n",
    "ig = IntGradInterpreter(model, device=\"gpu\")\n",
    "for idx, example in enumerate(dev_ds):\n",
    "    if idx % 1000 == 0:\n",
    "        print(idx)\n",
    "    interp_results += ig(preprocess_fn(example), steps=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate sentence pair map scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map_scores mean: 0.5252031447275877\n",
      "map_scores median: 0.5311459836459835\n",
      "map_scores min: 0.0\n",
      "map_scores max: 0.9795918367346939\n"
     ]
    }
   ],
   "source": [
    "from interpretation.token_level.common import get_rationales_and_non_ratioanles\n",
    "from evaluation import Evaluator\n",
    "\n",
    "evaluator = Evaluator()\n",
    "map_scores = []\n",
    "for idx, example in enumerate(dev_ds):\n",
    "    text_a, text_b = example['query'], example['title']\n",
    "\n",
    "    # get subword\n",
    "    subwords_a = tokenizer.tokenize(text_a)\n",
    "    subwords_b = tokenizer.tokenize(text_b)\n",
    "\n",
    "    # calculate attributions individually\n",
    "    attributions = interp_results[idx].attributions\n",
    "    attributions_a = attributions[1:len(subwords_a) + 1]\n",
    "    attributions_b = attributions[len(subwords_a) + 2:len(subwords_a) + len(subwords_b) + 2]\n",
    "\n",
    "    # sorted subword by attributions\n",
    "    sorted_tokens_a = [subwords_a[i] for i in sorted(range(len(subwords_a)), key=lambda j : attributions_a[j], reverse=False)]\n",
    "    sorted_tokens_b = [subwords_b[i] for i in sorted(range(len(subwords_b)), key=lambda j : attributions_b[j], reverse=False)]\n",
    "\n",
    "    # map score\n",
    "    map_score_a = evaluator._calc_map_by_bin(sorted_tokens_a, sorted_tokens_b)\n",
    "    map_score_b = evaluator._calc_map_by_bin(sorted_tokens_b, sorted_tokens_a)\n",
    "    map_scores.append((map_score_a + map_score_b) / 2)\n",
    "print(\"map_scores mean:\", np.mean(map_scores))\n",
    "print(\"map_scores median:\", np.median(map_scores))\n",
    "print(\"map_scores min:\", np.min(map_scores))\n",
    "print(\"map_scores max:\", np.max(map_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the data under different thresholds and calculate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thresholds: 0.0 data size: 0 acc: 1\n",
      "thresholds: 0.1 data size: 4 acc: 1.0\n",
      "thresholds: 0.2 data size: 60 acc: 0.9666666666666667\n",
      "thresholds: 0.30000000000000004 data size: 303 acc: 0.7986798679867987\n",
      "thresholds: 0.4 data size: 921 acc: 0.8089033659066233\n",
      "thresholds: 0.5 data size: 1898 acc: 0.8261327713382508\n",
      "thresholds: 0.6000000000000001 data size: 3010 acc: 0.8528239202657807\n",
      "thresholds: 0.7000000000000001 data size: 3914 acc: 0.8730199284619315\n",
      "thresholds: 0.8 data size: 4392 acc: 0.8825136612021858\n",
      "thresholds: 0.9 data size: 4516 acc: 0.8844109831709478\n",
      "thresholds: 1.0 data size: 4530 acc: 0.8841059602649006\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "right = 0\n",
    "for i in np.linspace(0, 1, 11):\n",
    "    count = 0\n",
    "    right = 0\n",
    "    for idx, example in enumerate(dev_ds):\n",
    "        if predict_results[idx] == 'positive' and map_scores[idx] <= i:\n",
    "            count += 1\n",
    "            if label_map[example['label']] == predict_results[idx]:\n",
    "                right += 1\n",
    "    print(\"thresholds:\", i, \"data size:\", count, \"acc:\", right / count if count != 0 else 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('zs_py39': conda)",
   "metadata": {
    "interpreter": {
     "hash": "acea1e9fb1ca687a228f6dc71ee62aa15fcb20ac41dec3d8a9e155f35234403c"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
